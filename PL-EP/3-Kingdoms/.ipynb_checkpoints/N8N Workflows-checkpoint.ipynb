{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N8N Workflows Analysis\n",
    "\n",
    "This notebook reads and analyzes the N8N_Workflows.csv file containing various AI automation workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "try:\n",
    "    df = pd.read_csv('N8N_Workflows.csv')\n",
    "    print(\"âœ… File loaded successfully!\")\n",
    "    print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "    print(f\"ğŸ“ Number of workflows: {len(df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ File 'N8N_Workflows.csv' not found. Please ensure it's in the same directory as this notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nğŸ“‹ First 5 rows of the dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"\\nğŸ“ˆ Dataset Info:\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(\"\\nğŸ“ Column names and data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"\\nğŸ” Data Types and Missing Values:\")\n",
    "df_info = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Null Count': df.isnull().sum(),\n",
    "    'Null Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "display(df_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Format Date Column to YYYY-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Date column to YYYY-MM-DD\n",
    "print(\"\\nğŸ“… Formatting Date column...\")\n",
    "print(\"Before formatting:\")\n",
    "print(df['Date'].head())\n",
    "\n",
    "# Convert to datetime and format\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print(\"\\nAfter formatting:\")\n",
    "print(df['Date'].head())\n",
    "\n",
    "# Check for any failed conversions\n",
    "failed_conversions = df['Date'].isna().sum()\n",
    "if failed_conversions > 0:\n",
    "    print(f\"\\nâš ï¸  Warning: {failed_conversions} dates could not be converted and were set to NaT\")\n",
    "else:\n",
    "    print(\"\\nâœ… All dates converted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for numerical columns\n",
    "print(\"\\nğŸ“Š Statistical Summary (Numerical Columns):\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary for categorical columns\n",
    "print(\"\\nğŸ“Š Summary for Categorical Columns:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nğŸ“Œ {col}:\")\n",
    "    print(f\"   Unique values: {df[col].nunique()}\")\n",
    "    if df[col].nunique() <= 15:  # Only show value counts if not too many unique values\n",
    "        print(f\"   Top values:\")\n",
    "        print(df[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of FINAL SCORE\n",
    "print(\"\\nğŸ“ˆ Distribution of FINAL SCORE:\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['FINAL SCORE'].value_counts().sort_index().plot(kind='bar', alpha=0.7)\n",
    "plt.title('Distribution of FINAL SCORE')\n",
    "plt.xlabel('Final Score')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average FINAL SCORE: {df['FINAL SCORE'].mean():.2f}\")\n",
    "print(f\"Highest FINAL SCORE: {df['FINAL SCORE'].max()}\")\n",
    "print(f\"Lowest FINAL SCORE: {df['FINAL SCORE'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by Origin\n",
    "print(\"\\nğŸ“Š Workflows by Origin:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "origin_counts = df['Origin'].value_counts()\n",
    "bars = plt.bar(range(len(origin_counts)), origin_counts.values, alpha=0.7)\n",
    "plt.title('Number of Workflows by Origin')\n",
    "plt.xlabel('Origin')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(len(origin_counts)), origin_counts.index, rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, origin_counts.values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "            str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Authors\n",
    "print(\"\\nğŸ‘¥ Top Authors by Number of Workflows:\")\n",
    "top_authors = df['Author'].value_counts().head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(top_authors)), top_authors.values, alpha=0.7)\n",
    "plt.title('Top 10 Authors by Number of Workflows')\n",
    "plt.xlabel('Author')\n",
    "plt.ylabel('Number of Workflows')\n",
    "plt.xticks(range(len(top_authors)), top_authors.index, rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, top_authors.values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "            str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Score by Origin\n",
    "print(\"\\nğŸ“Š Average FINAL SCORE by Origin:\")\n",
    "avg_score_by_origin = df.groupby('Origin')['FINAL SCORE'].mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(avg_score_by_origin)), avg_score_by_origin.values, alpha=0.7)\n",
    "plt.title('Average FINAL SCORE by Origin')\n",
    "plt.xlabel('Origin')\n",
    "plt.ylabel('Average FINAL SCORE')\n",
    "plt.xticks(range(len(avg_score_by_origin)), avg_score_by_origin.index, rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, avg_score_by_origin.values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "            f'{score:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Analysis - Workflow Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common categories from titles\n",
    "print(\"\\nğŸ” Analyzing Workflow Categories from Titles:\")\n",
    "\n",
    "# Common AI/automation categories\n",
    "categories = {\n",
    "    'AI Agent': ['ai agent', 'agent', 'assistant'],\n",
    "    'Content Generation': ['content', 'generate', 'writer', 'creator'],\n",
    "    'Social Media': ['social', 'twitter', 'linkedin', 'instagram', 'tiktok', 'facebook'],\n",
    "    'Video': ['video', 'reel', 'shorts', 'youtube', 'tiktok'],\n",
    "    'Email': ['email', 'gmail', 'outlook'],\n",
    "    'Scraping': ['scrape', 'scraper', 'crawl'],\n",
    "    'Lead Generation': ['lead', 'prospect', 'outreach'],\n",
    "    'Sales': ['sales', 'cold', 'crm'],\n",
    "    'Marketing': ['marketing', 'ad', 'campaign'],\n",
    "    'Analysis': ['analyze', 'analysis', 'insight', 'report']\n",
    "}\n",
    "\n",
    "# Count occurrences of each category\n",
    "category_counts = {}\n",
    "for category, keywords in categories.items():\n",
    "    count = 0\n",
    "    for title in df['Title'].str.lower():\n",
    "        if any(keyword in title for keyword in keywords):\n",
    "            count += 1\n",
    "    category_counts[category] = count\n",
    "\n",
    "# Plot category distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "sorted_categories = dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "bars = plt.barh(range(len(sorted_categories)), list(sorted_categories.values()), alpha=0.7)\n",
    "plt.title('Workflow Categories (Based on Title Keywords)')\n",
    "plt.xlabel('Number of Workflows')\n",
    "plt.yticks(range(len(sorted_categories)), list(sorted_categories.keys()))\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, count) in enumerate(zip(bars, sorted_categories.values())):\n",
    "    plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "            str(count), ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"\\nğŸ” Data Quality Check:\")\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Check for empty strings\n",
    "empty_strings = (df.applymap(lambda x: x == '')).sum().sum()\n",
    "print(f\"Number of empty strings: {empty_strings}\")\n",
    "\n",
    "# Check for whitespace-only strings\n",
    "whitespace_only = (df.applymap(lambda x: isinstance(x, str) and x.strip() == '')).sum().sum()\n",
    "print(f\"Number of whitespace-only strings: {whitespace_only}\")\n",
    "\n",
    "# Check Setup Support column\n",
    "setup_support_stats = df['Setup Support'].value_counts(dropna=False)\n",
    "print(f\"\\nğŸ“ Setup Support distribution:\")\n",
    "print(setup_support_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Top Rated Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top rated workflows\n",
    "print(\"\\nğŸ† Top Rated Workflows (FINAL SCORE = 5):\")\n",
    "top_workflows = df[df['FINAL SCORE'] == 5][['Title', 'Author', 'Origin', 'Date']]\n",
    "display(top_workflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most recent workflows\n",
    "print(\"\\nğŸ†• Most Recent Workflows:\")\n",
    "recent_workflows = df.sort_values('Date', ascending=False).head(10)[['Title', 'Author', 'Origin', 'Date', 'FINAL SCORE']]\n",
    "display(recent_workflows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Cleaned Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to save cleaned data\n",
    "save_cleaned = input(\"\\nğŸ’¾ Do you want to save a cleaned version of the data? (y/n): \")\n",
    "\n",
    "if save_cleaned.lower() == 'y':\n",
    "    # Create cleaned dataframe\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Remove completely empty columns if any\n",
    "    cleaned_df = cleaned_df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Save to new CSV\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f'N8N_Workflows_cleaned_{timestamp}.csv'\n",
    "    cleaned_df.to_csv(filename, index=False)\n",
    "    print(f\"âœ… Cleaned data saved as: {filename}\")\n",
    "    \n",
    "    # Also save top workflows separately\n",
    "    top_workflows_filename = f'N8N_Top_Workflows_{timestamp}.csv'\n",
    "    df[df['FINAL SCORE'] >= 4][['Title', 'Description', 'Author', 'Origin', 'FINAL SCORE', 'Date', 'Link']].to_csv(top_workflows_filename, index=False)\n",
    "    print(f\"âœ… Top workflows (score >=4) saved as: {top_workflows_filename}\")\n",
    "else:\n",
    "    print(\"âŒ Data export cancelled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ N8N WORKFLOWS ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“Š Total Workflows: {df.shape[0]:,}\")\n",
    "print(f\"ğŸ“ Total Columns: {df.shape[1]}\")\n",
    "print(f\"â­ Average FINAL SCORE: {df['FINAL SCORE'].mean():.2f}/5\")\n",
    "print(f\"ğŸ† Top Rated Workflows (5/5): {(df['FINAL SCORE'] == 5).sum()}\")\n",
    "print(f\"ğŸ‘¥ Unique Authors: {df['Author'].nunique()}\")\n",
    "print(f\"ğŸŒ Platforms: {', '.join(df['Origin'].unique())}\")\n",
    "print(f\"ğŸ“… Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"âš ï¸  Missing Values: {df.isnull().sum().sum()}\")\n",
    "print(f\"ğŸ” Duplicate Rows: {df.duplicated().sum()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Top 3 authors\n",
    "top_authors_summary = df['Author'].value_counts().head(3)\n",
    "print(\"\\nğŸ‘‘ Top 3 Most Prolific Authors:\")\n",
    "for author, count in top_authors_summary.items():\n",
    "    print(f\"   {author}: {count} workflows\")\n",
    "\n",
    "# Platform distribution\n",
    "print(\"\\nğŸŒ Workflow Distribution by Platform:\")\n",
    "for origin, count in df['Origin'].value_counts().items():\n",
    "    percentage = (count / len(df) * 100)\n",
    "    print(f\"   {origin}: {count} workflows ({percentage:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}