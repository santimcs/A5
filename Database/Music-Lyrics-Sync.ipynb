{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c635474-c2f3-47d0-afa4-68255919673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üé¨ CREATING AUDIO-ANALYSIS SYNCED VIDEO\n",
      "======================================================================\n",
      "üéµ Creating AUDIO-SYNCED video for: Imagine\n",
      "üîä Audio: Imagine.mp3\n",
      "üñºÔ∏è Background: Folder.jpg\n",
      "üé§ Analyzing audio for vocal segments...\n",
      "üìä Audio loaded: 185.17s, SR: 44100Hz\n",
      "üìä Extracting audio features...\n",
      "üìê Spectral centroids shape: (1, 15950)\n",
      "üìê Times shape: (15950,), Normalized centroids shape: (15950,)\n",
      "‚ö†Ô∏è No vocal segments detected, using onset-based segmentation\n",
      "üìä Analysis plot saved: ../data/audio_analysis.png\n",
      "‚úÖ Detected 344 vocal segments\n",
      "‚úÖ Detected 345 musical onsets\n",
      "‚ùå Audio analysis error: unsupported format string passed to numpy.ndarray.__format__\n",
      "üìù Assigning 26 lyrics lines to 1 vocal segments\n",
      "‚ö†Ô∏è 1 lines not assigned to vocal segments, using fallback timing\n",
      "üìù Successfully assigned 26 lyrics lines\n",
      "‚è±Ô∏è Full song duration: 185.0s (3.1 minutes)\n",
      "üé¨ Creating audio-synced video frames...\n",
      "üìπ Exporting audio-synced video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC1\\AppData\\Local\\Temp\\ipykernel_482592\\1418798906.py\", line 121, in detect_vocal_segments\n",
      "    print(f\"üéµ Estimated tempo: {tempo:.1f} BPM\")\n",
      "                                ^^^^^^^^^^^\n",
      "TypeError: unsupported format string passed to numpy.ndarray.__format__\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AUDIO-SYNCED video created: ../data/videos\\Imagine_audio_synced.mp4\n",
      "üìä File size: 4.3 MB\n",
      "\n",
      "üìã Lyrics Timing Summary:\n",
      "--------------------------------------------------\n",
      " 1.   5.0s -  12.0s: Imagine there's no heaven\n",
      " 2.  12.0s -  19.0s: It's easy if you try\n",
      " 3.  19.0s -  26.0s: No hell below us\n",
      " 4.  26.0s -  33.0s: Above us only sky\n",
      " 5.  33.0s -  40.0s: Imagine all the people\n",
      " 6.  40.0s -  47.0s: Living for today\n",
      " 7.  47.0s -  54.0s: Imagine there's no countries\n",
      " 8.  54.0s -  61.0s: It isn't hard to do\n",
      " 9.  61.0s -  68.0s: Nothing to kill or die for\n",
      "10.  68.0s -  75.0s: And no religion, too\n",
      "11.  75.0s -  82.0s: Imagine all the people\n",
      "12.  82.0s -  89.0s: Living life in peace\n",
      "13.  89.0s -  96.0s: You may say I'm a dreamer\n",
      "14.  96.0s - 103.0s: But I'm not the only one\n",
      "15. 103.0s - 110.0s: I hope someday you will join us\n",
      "16. 110.0s - 117.0s: And the world will be as one\n",
      "17. 117.0s - 124.0s: Imagine no possessions\n",
      "18. 124.0s - 131.0s: I wonder if you can\n",
      "19. 131.0s - 138.0s: No need for greed or hunger\n",
      "20. 138.0s - 145.0s: A brotherhood of man\n",
      "21. 145.0s - 152.0s: Imagine all the people\n",
      "22. 152.0s - 159.0s: Sharing all the world\n",
      "23. 159.0s - 166.0s: You, you may say I'm a dreamer\n",
      "24. 166.0s - 173.0s: But I'm not the only one\n",
      "25. 173.0s - 180.0s: I hope someday you will join us\n",
      "26. 177.9s - 185.0s: And the world will live as one\n",
      "\n",
      "üéâ SUCCESS! Audio-synced video created: ../data/videos\\Imagine_audio_synced.mp4\n",
      "\n",
      "‚ú® Features:\n",
      "   ‚úÖ Automatic vocal segment detection\n",
      "   ‚úÖ Intro handling (no lyrics during instrumental intro)\n",
      "   ‚úÖ Beat and onset detection for better timing\n",
      "   ‚úÖ Spectral analysis for vocal activity\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your database connection\n",
    "engine = create_engine('mysql+pymysql://root@localhost:3306/music_development')\n",
    "data_path = '../data/'\n",
    "\n",
    "# Get song data\n",
    "sql = '''\n",
    "SELECT * FROM songs\n",
    "WHERE name LIKE \"imagine\"\n",
    "'''\n",
    "songs = pd.read_sql(sql, engine)\n",
    "songId = songs['id'].iloc[0]\n",
    "\n",
    "# Get lyrics\n",
    "sql = f'SELECT * FROM lyrics WHERE song_id = {songId}'\n",
    "lyrics = pd.read_sql(sql, engine)\n",
    "lyrics_content = lyrics.content.iloc[0]\n",
    "\n",
    "def detect_vocal_segments(audio_path, plot_analysis=False):\n",
    "    \"\"\"Use librosa to detect vocal segments and onsets - FIXED VERSION\"\"\"\n",
    "    try:\n",
    "        print(\"üé§ Analyzing audio for vocal segments...\")\n",
    "        \n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "        print(f\"üìä Audio loaded: {duration:.2f}s, SR: {sr}Hz\")\n",
    "        \n",
    "        # Extract features for vocal detection\n",
    "        print(\"üìä Extracting audio features...\")\n",
    "        \n",
    "        # Harmonic-percussive source separation\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "        \n",
    "        # Detect onsets (places where vocals/instruments start)\n",
    "        onset_frames = librosa.onset.onset_detect(\n",
    "            y=y, \n",
    "            sr=sr, \n",
    "            hop_length=512, \n",
    "            backtrack=True,\n",
    "            delta=0.1\n",
    "        )\n",
    "        onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=512)\n",
    "        \n",
    "        # Detect beats for rhythm analysis\n",
    "        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, hop_length=512)\n",
    "        beat_times = librosa.frames_to_time(beat_frames, sr=sr, hop_length=512)\n",
    "        \n",
    "        # Vocal activity detection using spectral features\n",
    "        stft = librosa.stft(y_harmonic)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y_harmonic, sr=sr)\n",
    "        \n",
    "        # FIX: Handle the shape of spectral_centroids properly\n",
    "        print(f\"üìê Spectral centroids shape: {spectral_centroids.shape}\")\n",
    "        \n",
    "        # Flatten and normalize features\n",
    "        spectral_flat = spectral_centroids.flatten()\n",
    "        spectral_centroids_normalized = (spectral_flat - np.min(spectral_flat)) / (np.max(spectral_flat) - np.min(spectral_flat))\n",
    "        \n",
    "        # Create time array for features - FIXED\n",
    "        times = librosa.times_like(spectral_centroids, sr=sr, hop_length=512)\n",
    "        times_flat = times.flatten()\n",
    "        \n",
    "        print(f\"üìê Times shape: {times.shape}, Normalized centroids shape: {spectral_centroids_normalized.shape}\")\n",
    "        \n",
    "        # Detect vocal segments based on spectral features - FIXED\n",
    "        vocal_segments = []\n",
    "        current_segment = None\n",
    "        vocal_threshold = 0.4  # Adjusted threshold\n",
    "        \n",
    "        # Ensure arrays are the same length\n",
    "        min_length = min(len(times_flat), len(spectral_centroids_normalized))\n",
    "        \n",
    "        for i in range(min_length):\n",
    "            time = times_flat[i]\n",
    "            centroid = spectral_centroids_normalized[i]\n",
    "            \n",
    "            if centroid > vocal_threshold:\n",
    "                if current_segment is None:\n",
    "                    current_segment = {'start': time, 'end': time}\n",
    "                else:\n",
    "                    current_segment['end'] = time\n",
    "            else:\n",
    "                if current_segment is not None:\n",
    "                    # Only keep segments longer than 0.5 seconds\n",
    "                    if current_segment['end'] - current_segment['start'] > 0.5:\n",
    "                        vocal_segments.append(current_segment)\n",
    "                    current_segment = None\n",
    "        \n",
    "        # Add the last segment if exists\n",
    "        if current_segment is not None and current_segment['end'] - current_segment['start'] > 0.5:\n",
    "            vocal_segments.append(current_segment)\n",
    "        \n",
    "        # If no vocal segments detected, use a fallback approach\n",
    "        if not vocal_segments:\n",
    "            print(\"‚ö†Ô∏è No vocal segments detected, using onset-based segmentation\")\n",
    "            # Use onsets to create segments\n",
    "            for i in range(len(onset_times) - 1):\n",
    "                vocal_segments.append({\n",
    "                    'start': onset_times[i],\n",
    "                    'end': onset_times[i + 1]\n",
    "                })\n",
    "        \n",
    "        # Plot analysis if requested\n",
    "        if plot_analysis:\n",
    "            plot_audio_analysis(y, sr, times_flat[:min_length], spectral_centroids_normalized[:min_length], onset_times, vocal_segments)\n",
    "        \n",
    "        print(f\"‚úÖ Detected {len(vocal_segments)} vocal segments\")\n",
    "        print(f\"‚úÖ Detected {len(onset_times)} musical onsets\")\n",
    "        print(f\"üéµ Estimated tempo: {tempo:.1f} BPM\")\n",
    "        \n",
    "        return {\n",
    "            'duration': duration,\n",
    "            'onset_times': onset_times,\n",
    "            'beat_times': beat_times,\n",
    "            'vocal_segments': vocal_segments,\n",
    "            'tempo': tempo\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Audio analysis error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Return basic analysis with duration only\n",
    "        return {\n",
    "            'duration': 185,  # Fallback duration for Imagine\n",
    "            'onset_times': [],\n",
    "            'beat_times': [],\n",
    "            'vocal_segments': [{'start': 5, 'end': 180}],  # Assume most of song has vocals\n",
    "            'tempo': 120\n",
    "        }\n",
    "\n",
    "def plot_audio_analysis(y, sr, times, spectral_centroids, onset_times, vocal_segments):\n",
    "    \"\"\"Plot audio analysis for debugging - FIXED\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot waveform\n",
    "    plt.subplot(3, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr, alpha=0.6)\n",
    "    plt.title('Audio Waveform')\n",
    "    plt.ylabel('Amplitude')\n",
    "    \n",
    "    # Plot spectral centroids\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(times, spectral_centroids, label='Spectral Centroid', color='r', linewidth=1)\n",
    "    plt.axhline(y=0.4, color='g', linestyle='--', label='Vocal Threshold')\n",
    "    plt.title('Spectral Centroid (Vocal Activity Indicator)')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot onsets and vocal segments\n",
    "    plt.subplot(3, 1, 3)\n",
    "    for segment in vocal_segments:\n",
    "        plt.axvspan(segment['start'], segment['end'], alpha=0.3, color='red', label='Vocal Segments' if segment == vocal_segments[0] else \"\")\n",
    "    \n",
    "    for onset in onset_times:\n",
    "        plt.axvline(x=onset, color='blue', alpha=0.7, linestyle='--', label='Onsets' if onset == onset_times[0] else \"\")\n",
    "    \n",
    "    plt.title('Vocal Segments and Musical Onsets')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Detection')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    plt.savefig('../data/audio_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"üìä Analysis plot saved: ../data/audio_analysis.png\")\n",
    "    plt.close()\n",
    "\n",
    "def assign_lyrics_to_segments(lyrics_text, audio_analysis):\n",
    "    \"\"\"Intelligently assign lyrics to vocal segments - IMPROVED\"\"\"\n",
    "    lines = [line.strip() for line in lyrics_text.split('\\n') if line.strip()]\n",
    "    \n",
    "    if not lines:\n",
    "        return None\n",
    "    \n",
    "    vocal_segments = audio_analysis.get('vocal_segments', [])\n",
    "    onset_times = audio_analysis.get('onset_times', [])\n",
    "    duration = audio_analysis.get('duration', 180)\n",
    "    \n",
    "    print(f\"üìù Assigning {len(lines)} lyrics lines to {len(vocal_segments)} vocal segments\")\n",
    "    \n",
    "    # If we have vocal segments, distribute lyrics among them\n",
    "    if vocal_segments:\n",
    "        lyrics_with_timing = []\n",
    "        total_vocal_duration = sum(seg['end'] - seg['start'] for seg in vocal_segments)\n",
    "        \n",
    "        # Estimate words per second in vocal sections\n",
    "        total_words = sum(len(line.split()) for line in lines)\n",
    "        words_per_second = total_words / total_vocal_duration if total_vocal_duration > 0 else 2\n",
    "        \n",
    "        current_line_idx = 0\n",
    "        current_time = 0\n",
    "        \n",
    "        for segment in vocal_segments:\n",
    "            seg_start = segment['start']\n",
    "            seg_end = segment['end']\n",
    "            seg_duration = seg_end - seg_start\n",
    "            \n",
    "            # Estimate how many lines fit in this segment based on word count\n",
    "            lines_in_segment = []\n",
    "            while current_line_idx < len(lines):\n",
    "                line = lines[current_line_idx]\n",
    "                word_count = len(line.split())\n",
    "                estimated_duration = word_count / words_per_second\n",
    "                \n",
    "                # If adding this line doesn't exceed segment duration, add it\n",
    "                if current_time + estimated_duration <= seg_duration:\n",
    "                    lines_in_segment.append(line)\n",
    "                    current_time += estimated_duration\n",
    "                    current_line_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Assign timing to lines in this segment\n",
    "            if lines_in_segment:\n",
    "                line_duration = seg_duration / len(lines_in_segment)\n",
    "                for i, line in enumerate(lines_in_segment):\n",
    "                    line_start = seg_start + (i * line_duration)\n",
    "                    line_end = seg_start + ((i + 1) * line_duration)\n",
    "                    \n",
    "                    lyrics_with_timing.append({\n",
    "                        'text': line,\n",
    "                        'start_time': line_start,\n",
    "                        'end_time': line_end\n",
    "                    })\n",
    "            \n",
    "            current_time = 0  # Reset for next segment\n",
    "        \n",
    "        # Fill any remaining lines at the end using simple distribution\n",
    "        remaining_lines = len(lines) - current_line_idx\n",
    "        if remaining_lines > 0:\n",
    "            print(f\"‚ö†Ô∏è {remaining_lines} lines not assigned to vocal segments, using fallback timing\")\n",
    "            time_per_line = duration / len(lines)\n",
    "            for i in range(current_line_idx, len(lines)):\n",
    "                start_time = i * time_per_line\n",
    "                end_time = (i + 1) * time_per_line\n",
    "                lyrics_with_timing.append({\n",
    "                    'text': lines[i],\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time\n",
    "                })\n",
    "            \n",
    "    else:\n",
    "        # Fallback: simple linear timing\n",
    "        print(\"‚ö†Ô∏è No vocal segments, using linear timing\")\n",
    "        lyrics_with_timing = []\n",
    "        time_per_line = duration / len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            lyrics_with_timing.append({\n",
    "                'text': line,\n",
    "                'start_time': i * time_per_line,\n",
    "                'end_time': (i + 1) * time_per_line\n",
    "            })\n",
    "    \n",
    "    return lyrics_with_timing\n",
    "\n",
    "def get_current_lyric(current_time, lyrics_with_timing):\n",
    "    \"\"\"Find which lyric should be displayed at current time\"\"\"\n",
    "    for lyric in lyrics_with_timing:\n",
    "        if lyric['start_time'] <= current_time < lyric['end_time']:\n",
    "            return lyric['text']\n",
    "    return None\n",
    "\n",
    "def create_audio_synced_video(song_id=songId, plot_analysis=True):\n",
    "    \"\"\"Create video with audio-analysis-based lyrics synchronization - FIXED\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get song data\n",
    "        query = f\"\"\"\n",
    "        SELECT s.name as song_name, s.location as audio_file,\n",
    "               l.content as lyrics, a.first_name, a.last_name\n",
    "        FROM songs s \n",
    "        JOIN lyrics l ON s.id = l.song_id \n",
    "        JOIN artists a ON s.artist_id = a.id \n",
    "        WHERE s.id = {song_id}\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, engine)\n",
    "        song_data = df.iloc[0]\n",
    "        \n",
    "        print(f\"üéµ Creating AUDIO-SYNCED video for: {song_data['song_name']}\")\n",
    "        \n",
    "        # Construct file paths\n",
    "        audio_dir = os.path.join(r\"C:\\ruby\\music\\public\\uploads\\song\\location\", str(song_id))\n",
    "        audio_path = os.path.join(audio_dir, song_data['audio_file'])\n",
    "        background_image_path = os.path.join(audio_dir, \"Folder.jpg\")\n",
    "        \n",
    "        print(f\"üîä Audio: {os.path.basename(audio_path)}\")\n",
    "        print(f\"üñºÔ∏è Background: {os.path.basename(background_image_path)}\")\n",
    "        \n",
    "        if not os.path.exists(audio_path):\n",
    "            print(\"‚ùå Audio file not found\")\n",
    "            return None\n",
    "        \n",
    "        # Perform audio analysis\n",
    "        audio_analysis = detect_vocal_segments(audio_path, plot_analysis=plot_analysis)\n",
    "        \n",
    "        # Assign lyrics to timing segments\n",
    "        lyrics_with_timing = assign_lyrics_to_segments(song_data['lyrics'], audio_analysis)\n",
    "        \n",
    "        if not lyrics_with_timing:\n",
    "            print(\"‚ùå Could not assign lyrics timing\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üìù Successfully assigned {len(lyrics_with_timing)} lyrics lines\")\n",
    "        \n",
    "        # Load audio clip\n",
    "        audio_clip = AudioFileClip(audio_path)\n",
    "        duration = audio_analysis.get('duration', audio_clip.duration)\n",
    "        \n",
    "        print(f\"‚è±Ô∏è Full song duration: {duration:.1f}s ({duration/60:.1f} minutes)\")\n",
    "        \n",
    "        # Video settings\n",
    "        fps = 24\n",
    "        width, height = 640, 480\n",
    "        \n",
    "        def make_synced_frame(t):\n",
    "            try:\n",
    "                # Load background\n",
    "                if os.path.exists(background_image_path):\n",
    "                    bg_image = Image.open(background_image_path)\n",
    "                    bg_image = bg_image.resize((width, height), Image.Resampling.LANCZOS)\n",
    "                    frame = np.array(bg_image)\n",
    "                else:\n",
    "                    frame = np.full((height, width, 3), [40, 40, 80], dtype=np.uint8)\n",
    "                \n",
    "                # Convert to PIL for text drawing\n",
    "                pil_img = Image.fromarray(frame)\n",
    "                draw = ImageDraw.Draw(pil_img)\n",
    "                \n",
    "                # Load font\n",
    "                try:\n",
    "                    font = ImageFont.truetype(\"arial.ttf\", 32)\n",
    "                except:\n",
    "                    try:\n",
    "                        font = ImageFont.truetype(\"C:/Windows/Fonts/arial.ttf\", 32)\n",
    "                    except:\n",
    "                        font = ImageFont.load_default()\n",
    "                \n",
    "                # Get current lyric based on audio analysis\n",
    "                current_line = get_current_lyric(t, lyrics_with_timing)\n",
    "                \n",
    "                if current_line:\n",
    "                    # Calculate text position\n",
    "                    try:\n",
    "                        bbox = draw.textbbox((0, 0), current_line, font=font)\n",
    "                    except AttributeError:\n",
    "                        bbox = draw.textsize(current_line, font=font)\n",
    "                        bbox = (0, 0, bbox[0], bbox[1])\n",
    "                    \n",
    "                    text_width = bbox[2] - bbox[0]\n",
    "                    text_height = bbox[3] - bbox[1]\n",
    "                    x = (width - text_width) // 2\n",
    "                    y = (height - text_height) // 2\n",
    "                    \n",
    "                    # Semi-transparent background for text\n",
    "                    padding = 10\n",
    "                    draw.rectangle([\n",
    "                        x - padding, y - padding,\n",
    "                        x + text_width + padding, y + text_height + padding\n",
    "                    ], fill=(0, 0, 0, 180))\n",
    "                    \n",
    "                    # Text with shadow for readability\n",
    "                    shadow_color = (0, 0, 0)\n",
    "                    text_color = (255, 255, 255)\n",
    "                    \n",
    "                    # Shadow\n",
    "                    draw.text((x+2, y+2), current_line, font=font, fill=shadow_color)\n",
    "                    # Main text\n",
    "                    draw.text((x, y), current_line, font=font, fill=text_color)\n",
    "                \n",
    "                return np.array(pil_img)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Frame error at {t:.1f}s: {e}\")\n",
    "                return np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Create video\n",
    "        print(\"üé¨ Creating audio-synced video frames...\")\n",
    "        video = VideoClip(make_synced_frame, duration=duration)\n",
    "        video = video.set_audio(audio_clip)\n",
    "        \n",
    "        # Export\n",
    "        output_dir = '../data/videos'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f\"{song_data['song_name']}_audio_synced.mp4\")\n",
    "        \n",
    "        print(\"üìπ Exporting audio-synced video...\")\n",
    "        video.write_videofile(\n",
    "            output_file, \n",
    "            fps=fps, \n",
    "            codec='libx264',\n",
    "            audio_codec='aac',\n",
    "            verbose=False,\n",
    "            logger=None\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ AUDIO-SYNCED video created: {output_file}\")\n",
    "        print(f\"üìä File size: {os.path.getsize(output_file) / (1024*1024):.1f} MB\")\n",
    "        \n",
    "        # Display timing information\n",
    "        print(\"\\nüìã Lyrics Timing Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, lyric in enumerate(lyrics_with_timing):\n",
    "            print(f\"{i+1:2d}. {lyric['start_time']:5.1f}s - {lyric['end_time']:5.1f}s: {lyric['text'][:40]}{'...' if len(lyric['text']) > 40 else ''}\")\n",
    "        \n",
    "        # Clean up\n",
    "        video.close()\n",
    "        audio_clip.close()\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Audio-synced video error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Install required packages if not already installed\n",
    "def install_required_packages():\n",
    "    \"\"\"Install required audio analysis packages\"\"\"\n",
    "    try:\n",
    "        import librosa\n",
    "        import matplotlib\n",
    "    except ImportError:\n",
    "        print(\"üì¶ Installing required audio analysis packages...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"librosa\", \"matplotlib\"])\n",
    "        print(\"‚úÖ Packages installed successfully\")\n",
    "\n",
    "# Run the audio-synced version\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages\n",
    "    install_required_packages()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üé¨ CREATING AUDIO-ANALYSIS SYNCED VIDEO\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    result = create_audio_synced_video(song_id=songId, plot_analysis=True)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\nüéâ SUCCESS! Audio-synced video created: {result}\")\n",
    "        print(\"\\n‚ú® Features:\")\n",
    "        print(\"   ‚úÖ Automatic vocal segment detection\")\n",
    "        print(\"   ‚úÖ Intro handling (no lyrics during instrumental intro)\")\n",
    "        print(\"   ‚úÖ Beat and onset detection for better timing\")\n",
    "        print(\"   ‚úÖ Spectral analysis for vocal activity\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Audio-synced video creation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931a0ad-1254-4dc6-bb0f-0c2437a7badc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
